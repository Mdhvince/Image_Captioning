{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, models\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#nltk.download('punkt')\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "from transform import *\n",
    "from custom_data import ImageCaptionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/captions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.captions = data.captions.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Two young guys with shaggy hair look at their hands while hanging out in the yard .',\n",
       " ' Two young , White males are outside near many bushes .',\n",
       " ' Two men in green shirts are standing in a yard .',\n",
       " ' A man in a blue shirt standing in a garden .',\n",
       " ' Two friends enjoy time spent together .']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "data.captions = (\n",
    "    data.captions.apply(lambda x: [list(i.split()) for i in x])\n",
    "                 .apply(lambda x: [[j.lower() for j in i] for i in x])\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_file(col_serie_pandas):\n",
    "    data_vocab = col_serie_pandas.apply(lambda x: ' '.join(x))\n",
    "    data_vocab = ' '.join(list(data_vocab))\n",
    "    vocabulary = word_tokenize(data_vocab)\n",
    "\n",
    "    words_tokens = set([i.lower() for i in vocabulary])\n",
    "    token_map_integer = {}\n",
    "\n",
    "    for n, i in enumerate(words_tokens):\n",
    "        token_map_integer[i] = n\n",
    "\n",
    "    token_map_integer['<start>'] = -400\n",
    "    token_map_integer['<end>'] = -401\n",
    "    \n",
    "    return vocabulary, token_map_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "f = open(\"mapping.pkl\",\"wb\")\n",
    "pickle.dump(token_map_integer,f)\n",
    "f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with open('mapping.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to transform the caption associated with the image into a list of word\n",
    "# tokenisation\n",
    "\n",
    "# iterate over all the training caption\n",
    "# create a dictionnary that maps all unique words to an Integer and these words : <start>, <end>\n",
    "words = ''.join(data.captions.values.flatten())\n",
    "word_tokenized = word_tokenize(words.lower())\n",
    "vocabulary = list(set(word_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_workers = 4\n",
    "csv_file = 'data/captions.csv'\n",
    "root_dir = 'data/flickr30k_images'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Rescale(224),\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.3\n",
    "\n",
    "def train_valid_split(training_set, validation_size):\n",
    "    \"\"\" Function that split our dataset into train and validation\n",
    "        given in parameter the training set and the % of sample for validation\"\"\"\n",
    "    \n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(training_set)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(validation_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    return train_sampler, valid_sampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set = ImageCaptionDataset(csv_file=csv_file,\n",
    "                                root_dir=root_dir,\n",
    "                                transform=transform)\n",
    "\n",
    "train_sampler, valid_sampler = train_valid_split(train_set, valid_size)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=valid_sampler,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "# obtain one batch of training images\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# display 10 images\n",
    "for i in np.arange(1):\n",
    "    \n",
    "    images, labels = batch['image'], batch['captions']\n",
    "    \n",
    "    #unormalize images\n",
    "    image = images[i].numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    \n",
    "    labels = labels[i]\n",
    "    \n",
    "    #plt.subplot(5,1,i+1)\n",
    "    plt.imshow(np.squeeze(image), cmap='gray')\n",
    "    display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EncoderCNN\n",
    "\n",
    "embed_size = 50\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA not available... Training on CPU')\n",
    "else:\n",
    "    print('CUDA available... Training on GPU')\n",
    "    \n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"input features: {encoder.embed.in_features}\")\n",
    "print(f\"output features (embed_size): {encoder.embed.out_features}\")\n",
    "assert encoder.embed.out_features==embed_size, \"The embbeding size doesn't match the output size of the Encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_on_gpu:\n",
    "    encoder.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
