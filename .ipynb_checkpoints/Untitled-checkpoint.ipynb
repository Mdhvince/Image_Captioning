{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, models\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#nltk.download('punkt')\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "from transform import *\n",
    "from custom_data import ImageCaptionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('data/results2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def create_vocab_file(col_serie_pandas):\n",
    "    data_vocab = col_serie_pandas.apply(lambda x: ' '.join(x))\n",
    "    data_vocab = ' '.join(list(data_vocab))\n",
    "    vocabulary = word_tokenize(data_vocab)\n",
    "\n",
    "    words_tokens = set([i.lower() for i in vocabulary])\n",
    "    token_map_integer = {}\n",
    "\n",
    "    for n, i in enumerate(words_tokens):\n",
    "        token_map_integer[i] = n\n",
    "\n",
    "    token_map_integer['<start>'] = -400\n",
    "    token_map_integer['<end>'] = -401\n",
    "    \n",
    "    return vocabulary, token_map_integer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "f = open(\"mapping.pkl\",\"wb\")\n",
    "pickle.dump(token_map_integer,f)\n",
    "f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with open('mapping.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 4\n",
    "csv_file = 'data/results2.csv'\n",
    "root_dir = 'data/flickr30k_images'\n",
    "mapper_file = 'mapping.pkl'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Rescale(224),\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.3\n",
    "\n",
    "def train_valid_split(training_set, validation_size):\n",
    "    \"\"\" Function that split our dataset into train and validation\n",
    "        given in parameter the training set and the % of sample for validation\"\"\"\n",
    "    \n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(training_set)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(validation_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    return train_sampler, valid_sampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set = ImageCaptionDataset(csv_file=csv_file,\n",
    "                                root_dir=root_dir,\n",
    "                                mapper_file=mapper_file,\n",
    "                                transform=transform)\n",
    "\n",
    "train_sampler, valid_sampler = train_valid_split(train_set, valid_size)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=valid_sampler,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[21, 21,  9],\n",
       "           [32, 34, 23],\n",
       "           [42, 43, 35],\n",
       "           ...,\n",
       "           [29, 29, 17],\n",
       "           [25, 28, 17],\n",
       "           [21, 26, 19]],\n",
       " \n",
       "          [[ 2,  1,  0],\n",
       "           [ 6,  7,  2],\n",
       "           [23, 24, 18],\n",
       "           ...,\n",
       "           [30, 28, 16],\n",
       "           [31, 27, 18],\n",
       "           [30, 26, 17]],\n",
       " \n",
       "          [[11, 11, 11],\n",
       "           [ 0,  0,  0],\n",
       "           [ 1,  1,  0],\n",
       "           ...,\n",
       "           [25, 27, 16],\n",
       "           [30, 28, 16],\n",
       "           [30, 28, 16]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[30, 15, 12],\n",
       "           [24, 15, 10],\n",
       "           [23, 13, 12],\n",
       "           ...,\n",
       "           [ 4,  4,  2],\n",
       "           [ 4,  4,  4],\n",
       "           [ 4,  4,  4]],\n",
       " \n",
       "          [[26, 12,  9],\n",
       "           [23, 16, 10],\n",
       "           [22, 10, 10],\n",
       "           ...,\n",
       "           [ 5,  5,  3],\n",
       "           [ 5,  5,  3],\n",
       "           [ 4,  4,  4]],\n",
       " \n",
       "          [[23, 12,  8],\n",
       "           [23, 15, 12],\n",
       "           [24, 12, 12],\n",
       "           ...,\n",
       "           [ 6,  6,  4],\n",
       "           [ 6,  6,  4],\n",
       "           [ 5,  5,  3]]]], dtype=torch.uint8),\n",
       " 'caption': [('a',),\n",
       "  ('baby',),\n",
       "  ('wearing',),\n",
       "  ('a',),\n",
       "  ('white',),\n",
       "  ('gown',),\n",
       "  ('waves',),\n",
       "  ('a',),\n",
       "  ('muslim',),\n",
       "  ('flag',),\n",
       "  ('.',)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-92dd6b3417b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#plt.subplot(5,1,i+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2701\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    644\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    645\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 646\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "# obtain one batch of training images\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# display 10 images\n",
    "for i in np.arange(1):\n",
    "    \n",
    "    images, labels = batch['image'], batch['caption']\n",
    "    \n",
    "    #unormalize images\n",
    "    image = images[i].numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    \n",
    "    labels = labels[i]\n",
    "    \n",
    "    #plt.subplot(5,1,i+1)\n",
    "    plt.imshow(np.squeeze(image), cmap='gray')\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from models import EncoderCNN\n",
    "\n",
    "embed_size = 50\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA not available... Training on CPU')\n",
    "else:\n",
    "    print('CUDA available... Training on GPU')\n",
    "    \n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"input features: {encoder.embed.in_features}\")\n",
    "print(f\"output features (embed_size): {encoder.embed.out_features}\")\n",
    "assert encoder.embed.out_features==embed_size, \"The embbeding size doesn't match the output size of the Encoder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if train_on_gpu:\n",
    "    encoder.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
