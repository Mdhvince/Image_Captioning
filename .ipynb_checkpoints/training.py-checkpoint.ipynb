{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, models\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "from transform import *\n",
    "from custom_data import ImageCaptionDataset\n",
    "from models import EncoderCNN, DecoderRNN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Calculating length...\n",
      "Reading Mapper file...\n",
      "Ready !\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_workers = 4\n",
    "csv_file = 'data/results2.csv'\n",
    "root_dir = 'data/flickr30k_images'\n",
    "mapper_file = 'mapping.pkl'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Rescale(224),\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "with open(mapper_file, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "    \n",
    "valid_size = 0.3\n",
    "\n",
    "\n",
    "def train_valid_split(training_set, validation_size):\n",
    "    \"\"\" Function that split our dataset into train and validation\n",
    "        given in parameter the training set and the % of sample for validation\"\"\"\n",
    "    \n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(training_set)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(validation_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    return train_sampler, valid_sampler\n",
    "\n",
    "\n",
    "train_set = ImageCaptionDataset(csv_file=csv_file,\n",
    "                                root_dir=root_dir,\n",
    "                                mapper_file=mapper_file,\n",
    "                                transform=transform)\n",
    "\n",
    "train_sampler, valid_sampler = train_valid_split(train_set, valid_size)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          sampler=train_sampler,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sampler=valid_sampler,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "embed_size = 256\n",
    "vocab_size = len(vocab)\n",
    "hidden_size = 512\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "# Move to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Learnable parameters & Optimizer\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "\n",
    "# This is to make sure that the 1st loss is  lower than sth and\n",
    "# Save the model according to this comparison\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    # Keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for data in train_loader:\n",
    "        images, captions = data['image'], data['caption']\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        images.to(device)\n",
    "        captions.to(device)\n",
    "        \n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*images.size(0)\n",
    "    \n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for data in valid_loader:\n",
    "        images, captions = data['image'], data['caption']\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        images.to(device)\n",
    "        captions.to(device)\n",
    "        \n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "        \n",
    "        # Average losses\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "        \n",
    "        print(f\"Epoch: {epoch} \\tTraining Loss: {train_loss} \\tValidation Loss: {valid_loss}\")\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f\"Validation loss decreased ({valid_loss_min} --> {valid_loss}).  Saving model ...\")\n",
    "            torch.save(encoder.state_dict(), f'saved_models/encoder{n_epochs}.pt')\n",
    "            torch.save(decoder.state_dict(), f'saved_models/decoder{n_epochs}.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
